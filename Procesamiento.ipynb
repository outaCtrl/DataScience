{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53e39db-2fb5-481e-82d5-42bdc8c9362c",
   "metadata": {},
   "source": [
    "# Procesamiento de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1ffab-2913-4a92-83fa-982ad5b97a1a",
   "metadata": {},
   "source": [
    "En este notebook se pueden observar los procedimientos realizados a los set de datos provistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fda6b-02da-410f-a0e5-ee877bdc2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d80e45-368d-4488-8fe5-e4f568478be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "#Stop Words de es_core_news_sm\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "stopwords_spacy = list(STOP_WORDS)\n",
    "\n",
    "# Se quitan de las stopwords palabras como 'no', 'bueno', 'buena' que pueden ser últiles para predecir\n",
    "stopwords_spacy.remove('no') \n",
    "stopwords_spacy.remove('bueno')\n",
    "stopwords_spacy.remove('buena')\n",
    "stopwords_spacy.remove('bien')\n",
    "stopwords_spacy.remove('buenos')\n",
    "stopwords_spacy.remove('tarde')\n",
    "stopwords_spacy.remove('temprano')\n",
    "stopwords_spacy.remove('día')\n",
    "stopwords_spacy.remove('días')\n",
    "stopwords_spacy.remove('dia')\n",
    "stopwords_spacy.remove('dias')\n",
    "stopwords_spacy.remove('grandes')\n",
    "stopwords_spacy.remove('general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e94a5-0cb4-436b-9f9c-c93c733b671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Words de nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_nltk = set(stopwords.words('spanish'))\n",
    "\n",
    "# Quito de las stopwords la palabra 'no', que nos puede ser últil para predecir\n",
    "stopwords_nltk.remove('no')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0cff4-d0ea-4628-aa4c-44ed7c409c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo dataset de desarrollo\n",
    "data_dev = pd.read_json('dataset_amazon/dataset_es_dev.json', lines = True)\n",
    "print(\"- Registros del set dev:\", data_dev.shape[0])\n",
    "\n",
    "# Importo dataset de entrenamiento\n",
    "data_train = pd.read_json('dataset_amazon/dataset_es_train.json', lines = True)\n",
    "print(\"- Registros del set train:\", data_train.shape[0])\n",
    "\n",
    "# Importo dataset de prueba\n",
    "data_test = pd.read_json('dataset_amazon/dataset_es_test.json', lines = True)\n",
    "print(\"- Registros del set test:\", data_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7719e-03e7-4d2e-8e6a-4ac03059363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concateno todos.\n",
    "data = pd.concat([data_dev, data_train, data_test])\n",
    "data = data.reset_index(drop=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316f596-022b-442b-8855-6118fcbaa536",
   "metadata": {},
   "source": [
    "#### Filtrado de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d2e01-4511-4d1e-9381-685104c30fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['product_category','review_title','review_body','stars']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc888043-8b86-4374-884c-d4a3acfadcd8",
   "metadata": {},
   "source": [
    "#### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f812fc-750b-41dc-bedb-9bac8132a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constante de signos de puntuación\n",
    "import string\n",
    "puntua = string.punctuation + '¡¿...'\n",
    "excluded_pos = ['SCONJ','CCONJ','NUM','PUNCT','PRON','DET','ADP','AUX','X','PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f72ec7-9180-4054-933d-42f9b4f8d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos con lemmatizer\n",
    "def text_data_lemma(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ not in excluded_pos and str(token) not in stopwords_spacy and len(token.text)>2): \n",
    "            temp = token.lemma_.strip()\n",
    "            clean_tokens.append(temp.lower())\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a97739-85a1-42d6-a988-c0b9fba2fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer # Permite stemmizar palabras en español\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "#Función para limpieza de datos con stemmer\n",
    "def text_data_stem(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ not in excluded_pos and str(token) not in stopwords_spacy and len(token.text)>2): \n",
    "            temp = stemmer.stem(token.text).strip()\n",
    "            clean_tokens.append(temp.lower())\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce641f3-9712-4c5c-9a48-6a9a32717d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatización (40 min aprox)\n",
    "\n",
    "# Limpiamos todas las reviews con lemmatizer\n",
    "reviews_lemma = []\n",
    "for i in df.index:\n",
    "    rev = text_data_lemma(df.review_title.iloc[i] + ' ' + df.review_body.iloc[i])\n",
    "    reviews_lemma.append(\" \".join(rev))\n",
    "reviews_lemma[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944260a-83dd-4014-82f7-11fbd685324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmización (40 min aprox)\n",
    "\n",
    "# Limpiamos todas las reviews con stemmizer\n",
    "reviews_stem = []\n",
    "for i in df.index:\n",
    "    rev = text_data_stem(df.review_title.iloc[i] + ' ' + df.review_body.iloc[i])\n",
    "    reviews_stem.append(\" \".join(rev))\n",
    "reviews_stem[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1fbe4-047a-4645-874e-e63126f3271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar nuevos datos en achivo json\n",
    "\n",
    "# Agregamos columna al dataset\n",
    "df['revs_lemma'] = reviews_lemma\n",
    "df['revs_stem'] = reviews_stem\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edf2da-9d9f-4700-9346-d93e6f734486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos dataset lemmatizado\n",
    "df.to_json(path_or_buf='dataset_amazon_clean.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
